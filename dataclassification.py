# -*- coding: utf-8 -*-
"""DataClassification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11xhP-p8mEbMNxIWg_w1OcTJfeCwD0AjZ
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from google.colab import drive
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import roc_curve, auc, precision_recall_fscore_support
from sklearn.metrics import classification_report,plot_confusion_matrix,accuracy_score
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2, f_classif
import matplotlib.pyplot as plt

drive.mount("/content/gdrive")

col_names = ['fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym',  'fM3Long', 'fM3Trans', 'fAlpha', 'fDist', 'class']

df = pd.read_csv('/content/gdrive/My Drive/magic/magic04.data', names = col_names)

g = df.loc[df['class'] == 'g']
h = df.loc[df['class'] == 'h']

df.shape

g.shape

h.shape

np.random.seed(10)

remove_n = g.shape[0] - h.shape[0]

drop_indices = np.random.choice(g.index, remove_n, replace=False)
g_subset = g.loc[drop_indices, :]
df = df.drop(drop_indices)

g.shape

h.shape

df.shape

gg = df.loc[df['class'] == 'g']
hh = df.loc[df['class'] == 'h']

gg.shape , hh.shape

x_before_spliting = df[col_names[:-1]]
y_before_spliting = df['class']

X_train, X_test, Y_train, Y_test = train_test_split(x_before_spliting, y_before_spliting, train_size=0.7)

X_train.shape, X_test.shape

Y_train.shape, Y_test.shape

def knn():
    neigh_score =[]
    for i in range(1,30,1):
        knn = KNeighborsClassifier(n_neighbors=i)
        knn.fit(X_train,Y_train)
        pred = knn.predict(X_test)
        score = accuracy_score(Y_test,pred)
        neigh_score.append((i, score))
    k = max(neigh_score,key=lambda x:x[1])[0]
    knn = KNeighborsClassifier(n_neighbors=k)
    return knn

def rand_forest():
    params =[]
    for i in ['gini','entropy']:
        for j in range(1,100,1):
            forest = RandomForestClassifier(n_estimators=j, max_depth=2,random_state=0,criterion=i)
            forest.fit(X_train,Y_train)
            pred = forest.predict(X_test)
            score = accuracy_score(Y_test,pred)
            params.append((i,j,score))
    heighest = max(params,key=lambda x:x[2])
    forest = RandomForestClassifier(n_estimators=heighest[1], max_depth=2,random_state=0,criterion=heighest[0])
    return forest

def ada_boost():
    params = []
    for i in range(1,100,1):
        clf = AdaBoostClassifier(n_estimators=i, random_state=0)
        clf.fit(X_train,Y_train)
        pred = clf.predict(X_test)
        score = accuracy_score(Y_test,pred)
        params.append((i,score))

    clf = AdaBoostClassifier(n_estimators=max(params,key=lambda x:x[1])[0], random_state=0)
    return clf

def dec_tree():
    tree = DecisionTreeClassifier(criterion='entropy')
    return tree

tunning = {"Naive Bayes":GaussianNB, "KNN": knn, "Random Forest": rand_forest, "Ada Boost": ada_boost, "Decision Tree": dec_tree}

for name in tunning:
    model = tunning[name]()
    y_pred = model.fit(X_train, Y_train).predict(X_train)
    
    print(name)
    print("Number of mislabeled points %d out of %d total points."% ((Y_train != y_pred).sum(), X_train.shape[0]))
    r = precision_recall_fscore_support(Y_train, y_pred)
    print("precision: ",str( (r[0][0]+r[0][1])/2 ), "\t Recall: ", str( (r[1][0]+r[1][1])/2 ), "\t FScore: ", str( (r[2][0]+r[2][1])/2 ))
    print("Model accuracy =" , model.score(X_test,Y_test))  
    plot_confusion_matrix(model,X_test,Y_test,normalize = 'true' )
    print("=======================================")